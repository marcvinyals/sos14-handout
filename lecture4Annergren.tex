% Copyright (C) 2014 by Massimo Lauria
% 
% Created   : "2014-01-07, Tuesday 17:01 (CET) Massimo Lauria"
% Time-stamp: "2014-01-28, 14:33 (CET) Massimo Lauria"
% Encoding  : UTF-8

% ---------------------------- USER DATA ------------------------------
\def\DataTitle{4. Semidefinite program relaxations of integer programs.}
\def\DataTitleShort{Linear programming}
\def\DataDate{4 February, 2014}
\def\DataDocname{Lecture 4 --- \DataDate}
\def\DataLecturer{Massimo Lauria}
\def\DataScribe{Mariette Annergren}
\def\DataKeywords{integer programming, linear programming,
  \Lovasz-\Schrijver, Sherali-Adams}

\def\DataAbstract{%
  We continue the discussion on how to improve the initial relaxation made of an integer program. Here, we focus on semidefinite program (SDP) relaxations. }


% ---------------------------- PREAMBLE -------------------------------
\documentclass[a4paper,twoside,justified]{tufte-handout}
\usepackage{soscourse} % this is a non standard package
\begin{document} 
% --------------------------- DOCUMENT --------------------------------

\section{Motivation for SDP relaxations}
SDP relaxations are more complicated to construct than linear relaxations, but more powerful. To see this, we consider the maximum cut problem.

\subsection{Maximum cut problem\cite{poljak1995maximumcut}}
We consider a finite, undirected and loopless graph $G(V,E)$, where $V$ denotes the set of vertexes and $E$ denotes the set of edges. Each edge is weighted according to some function $c:E\rightarrow \RR$. 

Let $S\subseteq V$, the \introduceterm{cut} is defined as the edges with one end in $S$ and the other end in $V\backslash S$ and the \introduceterm{weight of the cut} is the sum of the weights corresponding to the edges belonging to the cut. 

The maximum cut problem is equivalent to the problem of finding the set $S$ that maximizes the weight of the induced cut. It is NP-complete\cite{karp1972NPhard}.

Several approximation algorithms have been developed to be able to solve it with a high \introduceterm{performance guarantee}\footnote{The performance guarantee is defined as
\begin{equation*}
\rho=\frac{\text{optimal value of relaxed problem}}{\text{optimal value of original problem}},
\end{equation*}
that is, the inverse of the integrality gap.} in polynomial time. The approximation algorithm with the highest performance guarantee is an SDP relaxation that achieves $\rho\approx 0.878$.

However, any Sherali-Adams relaxation of degree $t=n^{s(\epsilon)}$ can only achieve a performance guarantee of $1/2 + \epsilon$\footnote{Citation needed}.

\section{Small example of SDP relaxation}
We want to show that a polynomial $p$ is non-negative over some polytope and write $p\geq 0$. If the polynomial $p$ is expressible as a sum of squared polynomials, then we can find such representation in a tractable manner by reformulating the problem as a SDP relaxation\footnote{Parrilo}. Let us see a small example. Consider
\begin{equation*}
F(x,y)=2x^4+2x^3y-x^2y^2+5y^4
\end{equation*}
and assume it is expressible as a sum of squared polynomials. These have degree at least 2, half the degree of $F$, and furthermore since $F$ is homogeneous they have degree exactly 2. The monomials in $(x,y)$ of degree 2 are $x^2$, $y^2$ and $xy$.

We can express sums of squares of polynomials with the following matrix operation:
\begin{align}
\label{matrixOperation}
&\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}^T
\underbrace{
\begin{pmatrix}
q_{11} & q_{12} & q_{13}\\
q_{21} & q_{22} & q_{23}\\
q_{31} & q_{32} & q_{33}
\end{pmatrix}
}_Q
\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}
=\notag\\
&q_{11}x^4+q_{22}y^4+(q_{33}+2q_{12})x^2y^2+2q_{13}x^3y+2q_{23}xy^3.
\end{align}
We match the coefficients in \eqref{matrixOperation} with those in $F(x,y)$, that is,
\begin{eqnarray*}
F(x,y)=\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}^T
Q
\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}
\end{eqnarray*}
for
\begin{eqnarray*}
Q=
\begin{pmatrix}
2 & -3 & 1\\
-3 & 5 & 0\\
1 & 0 & 5
\end{pmatrix}.
\end{eqnarray*}
To construct the sum of squared polynomials yielding $F(x,y)$, we perform a Cholesky factorization\footnote{Need SDP?} of the matrix $Q$ in the following way:
\begin{align*}
&F(x,y)=\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}^T
U^TU
\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}
=\\
&\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}^T
\frac{1}{\sqrt{2}}
\begin{pmatrix}
2 & -3 & 1\\
0 & 1 & 3\\
0 & 0 & 0
\end{pmatrix}^T
\frac{1}{\sqrt{2}}
\begin{pmatrix}
2 & -3 & 1\\
0 & 1 & 3\\
0 & 0 & 0
\end{pmatrix}
\begin{pmatrix}
x^2\\
y^2\\
xy
\end{pmatrix}=\\
&\bigg(\frac{1}{\sqrt{2}}\big(2x^2-3y^2+xy\big)\bigg)^2+\bigg(\frac{1}{\sqrt{2}}\big(y^2+3xy\big)\bigg)^2.
\end{align*}
We have reformulated $F(x,y)$ as a sum of squared polynomials and, consequently, we have shown that $F(x,y)\geq0$.

It is important to note that not every polynomial $p\geq 0$ on $\RR^n$ can be expressed as a sum of squared polynomials if $n\geq2$. An example of this is the Motzkin polynomial\cite{motzkin1967sos},
\begin{align*}
M(x,y,z)=x^6+y^4z^2+y^2z^4-3x^2y^2z^2.
\end{align*}

\section{Moment matrix\cite{lasserre2001globalOpt}}
We consider polynomials $p(x):\RR^n\rightarrow\RR$ of degree $t$. We can express them as
\begin{eqnarray*}
p(x)=\sum_{\alpha}p_{\alpha}x^{\alpha},\ \text{with}\   x^{\alpha}=x_1^{\alpha_1}x_2^{\alpha_2}\cdots x_n^{\alpha_n},\ \text{and}\  \sum_{i=1}^n\alpha_i\leq t,
\end{eqnarray*}
where 
\begin{align}
\label{basis}
1,x_1,x_2,\ldots,x_n,x_1^2,x_1x_2,\ldots,x_1x_n,\ldots,x_n^2,\dots,x_1^t,\ldots,x_n^t
\end{align}
is a basis of $p(x)$ and $p$ is the coefficient vector of $p(x)$ with respect to \eqref{basis}. 

{\Huge Is this just wrong?}

We introduce the auxillary variable $y_{00\ldots 0}=1$ and perform the variable substitution $x_1^ix_2^j\cdots x_n^l=y_{ij\ldots l}$. The moment matrix $M_t(y)$ has rows and columns labelled according to the basis \eqref{basis} and its elements are the corresponding $y_{ij\ldots l}$. For simplicity, we only show the case for $n=2$ and $t=2$, 
\begin{eqnarray*}
M_2(y)=
\bordermatrix{~ & 1            & x_1     &x_2      &x_1^2 &x_1x_2&x_2^2 \cr
                         1 & 1           & y_{10} & y_{01}&y_{20}&y_{11}&y_{02} \cr
                     x_1 & y_{10}  & y_{20} & y_{11}&y_{30}&y_{21}&y_{12} \cr
                     x_2 & y_{01}  & y_{11} & y_{02}&y_{21}&y_{12}&y_{03} \cr
                x_1^2 & y_{20}  & y_{30} & y_{21}&y_{40}&y_{31}&y_{22} \cr
             x_1x_2  & y_{11}  & y_{21} & y_{12}&y_{31}&y_{22}&y_{13} \cr
            x_2^2     & y_{02}  & y_{12} & y_{30}&y_{22}&y_{13}&y_{04} \cr}.
\end{eqnarray*}

Consider the squared polynomial
\begin{eqnarray*}
p^2(x)=(\sum_{\alpha}p_{\alpha}x^{\alpha})^2=\sum_{\alpha}\sum_{\alpha'}p_{\alpha}x^{\alpha}p_{\alpha'}x^{\alpha'}.
\end{eqnarray*}
It can be expressed, using the moment matrix, in the following way:
\begin{eqnarray*}
p^2(x)=p_{\alpha}^TM_t(y)P_{\alpha}.
\end{eqnarray*}
Consequently, we have that $p^2(x)\geq0\Leftrightarrow M_t(y)\succeq0$.

\section{\Lovasz-\Schrijver hierarchy with SDP}
 LS$_+$\footnote{The \Lovasz-\Schrijver hierarchy with SDP.} is a lift and project relaxation as its linear version. We are still limited to start each iteration with linear inequalities and we are only allowed to introduce quadratic inequalities. In fact, the only difference is that the matrix constraint $Y\succeq0$ has been added.

\subsection{Geometric interpretation}
As for the \Lovasz-\Schrijver hierarchy, we introduce the auxiliary variable $x_0=1$ and perform the variable substitution $x_ix_j=Y_{ij}$ for every $i,j=0,\ldots,n$. We let any terms of degree zero be represented by $Y_{00}$. We enforce the following constraints on $Y$:
\begin{enumerate}
\item \label{lsp-symmetry} $Y=Y^T$,
\item \label{lsp-eatsquares} $y_{0i}=y_{i0}=y_{ii}, \quad\text{for}\ i=0,\ldots,n$,
\item \label{lsp-multiplyx} $Y^{(i)}\in P \quad\text{for}\ i=0,\ldots,n$,
\item \label{lsp-multiplyoneminusx} $Y^{(0)}-Y^{(i)}\in P \quad\text{for}\ i=0,\ldots,n$.
\item \label{lsp-sdp} $Y\succeq0$.
\end{enumerate}
Constraints~\ref{lsp-symmetry} and~\ref{lsp-eatsquares} are due to that $x_ix_j=x_jx_i$ and $x_i-x_i^2=0$. Constraints~\ref{lsp-multiplyx} and~\ref{lsp-multiplyoneminusx} stem from 
\[\sum_{j=1}^{n} A^{(j)}Y_{ij}-bY_{i0}\leq 0, \quad\text{for}\ i=0,\ldots,n,\]
and 
\[\sum_{j=1}^{n} A^{(j)}(Y_{0j}-Y_{ij})-b(Y_{00}-Y_{i0})\leq 0, \quad\text{for}\ i=0,\ldots,n,\]
respectively\footnote{Uh??}. The matrix constraint~\ref{lsp-sdp} is equivalent to $p^2(Y)\geq0$ for all polynomials $p(\cdot)$\footnote{undef?} of degree one as discussed previously\footnote{I CAN'T REFERENCE SECTIONS BY NUMBER. AAARGH!}.

\subsection{Operator and rank}
The operation $N_+(P)$ is the projection over $1=Y_{00}$, $x_1=Y_{01}$,\ldots,$x_n=Y_{0n}$. The rank $t$ of a proof in LS$_+$ is the same as for \Lovasz-\Schrijver hierarchy. Note that, if $P$ is the polytope corresponding to the set of initial inequalities then $N_+(P)$ is determined by inequalities derivable in rank one. 

The projection $N_+^t(P)$ can be solved in $n^{\mathcal{O}(t)}$ given a polynomial time separator of $P$. We can use the same separator as for $N^t(P)$, with the additional requirement that $Y\succeq0$.

\begin{example}
Consider the following problem:
\begin{alignat}{2}
\label{example1}
  \maximize \sum_{u\in V} x_u\notag \\
  \subjectto G=(V,E)=K_n\notag \\
  & x_u+x_v\leq 1,\notag \\
&(u,v)\in E\notag\\
&x_u\in\{0,1\}.
\end{alignat}
We want to show that the objective value of \eqref{example1} is at most one, this is equivalent to showing that $\sum x_u\leq 1$. Using LS$_+$, we can do this in rank one.

The axioms of the proof are $1-x_u-x_v\geq 0$, $x_u^2-x_u=0$, and we can derive
\begin{alignat*}{2}
&\sum_u\sum_{v\neq u}(1-x_u-x_v)x_u+\sum_u(x_u^2-x_u)(n-2)+(1-\sum_ux_u)^2\geq0\\
&\Rightarrow1-\sum_ux_u\geq0.
\end{alignat*}
\end{example}

\section{Positivstellensatz proof systems\cite{grigoriev2001complexity}}
Positivstellensatz proof systems are, as the name implies, based on the Positivstellensatz\cite{bochnak1987satz}. 

We consider a system of constraints
\begin{equation}
\label{problemsatz}
f_1=0,\ldots,f_k=0,\ h_1\geq0,\ldots,h_m\geq0,
\end{equation}
where $f_i$ and $h_i$ are polynomials of $x_1,\ldots,x_n$, and we require $0\leq x_i \leq 1$ and $x_i^2-x_i=0$. We introduce the following pair of polynomials:
\begin{equation}
\label{polpair}
f = \sum_{s=1}^kf_sg_s,\quad h = \sum_{I\subseteq\{1,\ldots,m\}}\big(\prod_{i\in I}h_i\big)\big(\sum_je^2_{I,j}\big),
\end{equation}
where $e_{I,j}$ and $g_s$ are arbitrary polynomials. If $f+h=-1$, we know that system \eqref{problemsatz} has no feasible solution because our initial constraints imply that $f=0$ and $g \geq 0$. Such a polynomial pair is called the \introduceterm{refutation} for \eqref{problemsatz}. The degree of the refutation is given by
\begin{equation}
\label{degreesatz}
\max_{s,I,j}\{\deg(f_s,g_s),\deg(e^2_{I,j}\prod_{i\in I}h_i)\}.
\end{equation} 

\section{Positivstellensatz calculus\cite{grigoriev2001complexity}}
Positivstellensatz calculus is the dynamic version of Positivstellensatz. Again our input is the system of constraints \eqref{problemsatz}, but now we let $f$ defined in \eqref{polpair} instead be an arbitrary polynomial derived from $f_1=0,\ldots,f_k=0$ and $x_i^2-x_i=0$ using the polynomial calculus inference rules\footnote{
The polynomial calculus inference rules are
\begin{equation*}
\frac{p \quad q}{\alpha p + \beta q} , \quad \frac{p}{xp}
\end{equation*}
for any polynomials $p,q$, coefficients $\alpha,\beta$ and variable $x$.
}. If $f+h=-1$, we call the polynomial pair the \introduceterm{Positivstellensatz calculus refutation} of \eqref{polpair}. The degree of the refutation is the maximum degree of $e^2_{I,j}\prod_{i\in I}h_i$ and of the derivation for $f$.

\section{Sum-of-squares hierarchy}
 The sum-of-squares hierarchy is a weaker version of the Positivstellensatz proof system where we do not allow to multiply constraints $h_j \geq 0$ together. We consider the same system \eqref{problemsatz}. We introduce the following polynomial:
\begin{eqnarray}
\label{polsos}
p=u_0+\sum_{j}u_jh_j+\sum_ig_if_i,
\end{eqnarray}
 where $u_j$ are sum-of-squares and $g_i$ are arbitrary polynomials. The degree of a proof is the maximal degree of $u_0$, $u_jh_j$ and $g_if_i$.

\subsection{Geometrical interpretation}

We start with $h_1\geq0,\ldots,h_m\geq0$ and a fixed $t$. As usual we lift the problem to a larger space and have the moment matrix $M^t=(y_{A \cup B})$ indexed by $|A|,|B| \leq t$. The constraints are to be \introduceterm{localized on moment matrices}, that is $M^t(h_i \circ Y)$. If $h_i = \sum_{S} \alpha_S \prod_{i\in S} x_i$ and we substitute $\prod_{i\in S}x_i$ for $y_S$, then $M^t(h_i \circ Y) = \sum_S \alpha_S (y_{A \cup B \cup S})$.

%Assume $t=1$ and let $h_1=x_1^2+3x_1$, then
%\begin{eqnarray*}
%M^1(h_1 \circ Y)=
%\begin{pmatrix}
%                        y_{20}+3y_{10}  & y_{30}+3y_{20}  & y_{21}+3y_{11}\cr
%               y_{30}+3y_{20}  & y_{40}+3y_{30} & y_{31}+3y_{21}\cr
%                 y_{21}+3y_{11} & y_{31}+3y_{21}& y_{22}+3y_{12}\cr
%\end{pmatrix}.
%\end{eqnarray*} 

We want to find a $y$ such that $M_t(y)\succeq0$ and $M_t(h_jy)\succeq0$ where the last matrix inequality is equivalent to $h_jp^2(y)$ for all polynomials $p$ of degree at most $t$.

We let $Q_t(K)$ denote the projection over $1=y_{\emptyset}$ and $x_i=y_{\{i\}}$ for all $i$, where $K$ is the set defined by the inequalities $h_1\geq0,\ldots,h_m\geq0$. 






% ------------------------- EPILOGUE ------------------------------
\bibliography{soscourse}
\bibliographystyle{alpha}

\end{document} 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
